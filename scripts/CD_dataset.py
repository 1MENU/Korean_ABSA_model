from util.utils import *
from base_data import *

def CD_dataset(raw_data, tokenizer, max_len):
    input_ids_list = []
    attention_mask_list = []
    token_type_ids_list = []
    token_labels_list = []

    polarity_input_ids_list = []
    polarity_attention_mask_list = []
    polarity_token_type_ids_list = []
    polarity_token_labels_list = []

    for utterance in raw_data:

        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)
        
        input_ids_list.extend(entity_property_data_dict['input_ids'])
        token_type_ids_list.extend(entity_property_data_dict['token_type_ids'])
        attention_mask_list.extend(entity_property_data_dict['attention_mask'])
        token_labels_list.extend(entity_property_data_dict['label'])

        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])
        polarity_token_type_ids_list.extend(polarity_data_dict['token_type_ids'])
        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])
        polarity_token_labels_list.extend(polarity_data_dict['label'])

    return TensorDataset(
        torch.tensor(input_ids_list),
        torch.tensor(token_type_ids_list),
        torch.tensor(attention_mask_list),
        torch.tensor(token_labels_list)
        ), TensorDataset(
            torch.tensor(polarity_input_ids_list), 
            torch.tensor(polarity_token_type_ids_list),
            torch.tensor(polarity_attention_mask_list),
            torch.tensor(polarity_token_labels_list)
        )


def tokenize_and_align_labels(tokenizer, form, annotations, max_len):

    entity_property_data_dict = {
        'input_ids': [],
        'token_type_ids' : [],
        'attention_mask': [],
        'label': []
    }
    polarity_data_dict = {
        'input_ids': [],
        'token_type_ids' : [],
        'attention_mask': [],
        'label': []
    }

    for pair in entity_property_pair:
        isPairInOpinion = False
        if pd.isna(form):
            break

        # ì´ ìžë¦¬ì— ì „ì²˜ë¦¬ ê°€ëŠ¥
        
        # form = replace_marks(form)
        
        tokenized_data = tokenizer(pair, form, padding='max_length', max_length=max_len, truncation=True)
        
        for annotation in annotations:
            entity_property = annotation[0]
            polarity = annotation[2]

            # # ë°ì´í„°ê°€ =ë¡œ ì‹œìž‘í•˜ì—¬ ìˆ˜ì‹ìœ¼ë¡œ ì¸ì •ëœê²½ìš°
            # if pd.isna(entity) or pd.isna(property):
            #     continue

            if polarity == '------------':
                continue


            if entity_property == pair:
                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])
                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
                entity_property_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
                entity_property_data_dict['label'].append(label_name_to_id['True'])

                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])
                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
                polarity_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
                polarity_data_dict['label'].append(polarity_name_to_id[polarity])

                isPairInOpinion = True
                break

        if isPairInOpinion is False:
            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])
            entity_property_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
            entity_property_data_dict['label'].append(label_name_to_id['False'])

    return entity_property_data_dict, polarity_data_dict


def get_CD_dataset(train_data, dev_data, test_data, pretrained_tokenizer):
    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer)
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

    train_CD_data, train_SC_data = CD_dataset(train_data, tokenizer, 256)
    dev_CD_data, dev_SC_data = CD_dataset(dev_data, tokenizer, 256)
    test_CD_data, test_SC_data = CD_dataset(test_data, tokenizer, 256)

    return train_CD_data, dev_CD_data, test_CD_data

"""
1. ì´ëª¨í‹°ì½˜ â†’ í…ìŠ¤íŠ¸ ëŒ€ì²´
    - Test set ì—ëŠ” ì´ëª¨í‹°ì½˜ ê°œìˆ˜ ìžì²´ëŠ” ì ìŒ
2. í…ìŠ¤íŠ¸ ì´ëª¨ì§€ (ex, :) , ^^, >.<, > 3 <, // _ //, ã…‹.ã…‹, (--)(__) )
    1. ì´ëª¨í‹°ì½˜
    2. ã…Žã…Žã…Ž í˜¹ì€ ã…‹ã…‹ã…‹
    
3. ~~ë°ì´í„° ì¦ê°•~~
4. ìŒì„±ì–´, ì˜íƒœì–´ ì²˜ë¦¬ 
    1. í† í¬ë‚˜ì´ì €ë¥¼ ëŒë ¤ë³´ì•˜ì„ ë•Œì˜ ê²°ê³¼ (ì•„ëž˜ ì°¸ê³ )
    2. 5ë²ˆ, 6ë²ˆ í¬í•¨
5. ì •ê·œí™”(normalization) : ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹, ã…Žã…Žã…Žã…Žã…Žã…Žã…Žã…Žã…Žã…Ž, ã… ã… ã… ã… ã… ã… ã… ã… ã… ã… ã… ã… ,  ;;;;;;;;;; ê°™ì€ê±° 
tokenize ë˜ëŠ” ì‚¬ì´ì¦ˆì— ë§žê²Œ í†µì¼í•˜ê¸°. (ê·¸ëƒ¥ì´ëž‘ / ## ë¶™ì–´ì„œ ë‚˜ì˜¤ëŠ” ë²„ì „ì´ëž‘ ë‚˜ëˆ ì„œ ex) ã…Žã…Žã…Žã…Ž, #ã…Žã…Ž) (ê·¼ë° ì´ê±° toknizerì— ë”°ë¼ ë‹¤ë¦„) â†’ KCELECTRA ì—ì„œ í•´ê²° ê°€ëŠ¥
6. ë°˜ë³µë˜ëŠ” ì–´ë¯¸ ( â€œê°•ì¶”ì¶”ì¶”ì¶”ì¶”â€¦â€)
7. # (í•´ì‹œí…Œê·¸), ì‚­ì œ
8. ë„ì–´ì“°ê¸°, ë§žì¶¤ë²• êµì •ê¸° api ì¨ì„œ ì „ì²˜ë¦¬
    1. ex) â€œë¶€ë“œëŸ¬ìš´ ìž´ëŠë‚Œì´ëž„ê¹Œâ€
    
    https://github.com/Beomi/KcELECTRA
    
"""

def replace_marks(sentence):
    """
    tokenization ì „
    strip() ì–‘ ëì˜ ëŒ€ìƒ ì œê±°
    re.sub() ëŒ€ìƒ ë³€ê²½. ê³µë°±ì´ë©´ ì‚­ì œ
    sentence = re.sub('', '', sentence)
    """
    print(sentence)
    # í…ìŠ¤íŠ¸ ì´ëª¨ì§€
    sentence = re.sub('\^\^', 'ã…‹ã…‹', sentence)
    sentence = re.sub(':\)', 'ã…‹ã…‹', sentence)
    sentence = re.sub('>.<', 'ã…‹ã…‹', sentence)
    sentence = re.sub('> 3 <', 'ã…‹ã…‹', sentence)
    sentence = re.sub('// _ //', 'ã…‹ã…‹', sentence)
    sentence = re.sub('ã…‹.ã…‹', 'ã…‹ã…‹', sentence)
    sentence = re.sub('\(--\)\(__\)', 'ã…‹ã…‹', sentence)
    sentence = re.sub('ðŸ’', 'â¤', sentence)
    sentence = re.sub('ã… ã……ã…œ', 'ã… ã… ', sentence)
    sentence = re.sub(':D', 'ã…Žã…Ž', sentence)
    sentence = re.sub('\+_\+/', 'ã…‹ã…‹', sentence)
    sentence = re.sub('\^-\^*', 'ã…Žã…Ž', sentence)
    sentence = re.sub('ã…Ž_ã…Ž', 'ã…Žã…Ž', sentence)
    sentence = re.sub('-_-', '', sentence)
    # sentence = re.sub('', '', sentence)
        
    # í•´ì‹œíƒœê·¸ ë°”ê¾¸ê¸°
    sentence = re.sub('#', ', ', sentence)
    

    # ðŸ‘ðŸ» ðŸ‘Œ ðŸ¤¡ðŸ‘  ðŸŽµ ðŸ°ðŸŽ‚ ðŸ™‹ðŸ» ðŸ™ðŸ» ð–¤âž° ðŸŒ¹ðŸ’‹ðŸ˜²ðŸ–’ðŸ’†â€â™€ðŸ˜¡ðŸ‘Œ ðŸ˜´ðŸ’§ðŸ™†â€â™‚ ðŸ˜ºðŸ™†â€â™‚ðŸ’†ðŸ»â€â™€ðŸ™†ðŸ»ðŸŒ»ðŸ˜®ðŸ¥ðŸŒ \\ devë°ì´í„°ì…‹
    # ðŸŒ¹ ðŸ‘¦ðŸ¼ ðŸ‘ðŸ»ðŸ‘ðŸ»ðŸ¤˜ðŸ’¡ðŸ¼ ðŸ˜²ðŸ™ƒðŸ± ðŸ•ºðŸ’ðŸ•·ðŸ•¸ðŸƒâ€â™€âœŒðŸ» ðŸ’‹ðŸ’„ðŸ“¸ðŸ’¯ðŸ’‹ðŸ‘ŒðŸš—ðŸ’¬ ðŸ¤®ðŸŽµðŸŽâž° ðŸ‘†ðŸ’Ž ðŸ·ðŸ˜œ ðŸ™†â€â™‚ðŸ–ðŸ’§ðŸ™‹ðŸ»â€â™€ // train
    # ì´ëª¨ì§€ í†µì¼/ê°ì†Œ
    sentence = re.sub('ðŸ‘ðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜„', 'ðŸ‘ ', sentence)
    sentence = re.sub('ðŸ–’', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ‘Œ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ¤¡', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ‘ ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸŽµ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ°', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸŽ‚', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ™‹ðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ™ðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ð–¤', 'ðŸ‘', sentence)
    sentence = re.sub('âž°', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸŒ¹', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’‹', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜²', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’†â€â™€', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜¡', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜´', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’§', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ™†â€â™‚', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜º', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’†ðŸ»â€â™€', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ™†ðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸŒ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜®', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ¥', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸŒ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ‘¦ðŸ¼', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ‘ðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ¤˜', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’¡', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ¼', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ˜²', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ™ƒ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ±', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ•º', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ•·', 'ì‹«ë‹¤ ', sentence)
    sentence = re.sub('ðŸ•¸', 'ì‹«ë‹¤ ', sentence)
    sentence = re.sub('ðŸƒâ€â™€', 'ðŸ‘', sentence)
    sentence = re.sub('âœŒðŸ»', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ’¯', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ¤®', 'ì‹«ë‹¤ ', sentence)
    sentence = re.sub('ðŸ˜œ', 'ðŸ‘', sentence)
    sentence = re.sub('ðŸ–', 'ðŸ‘', sentence)
    
    # ë°˜ë³µ ë¬¸ìž ì‚­ì œ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹, ã…Žã…Žã…Žã…Žã…Žã…Ž, ê°•ì¶”ì¶”ì¶”ì¶”ì¶”
    sentence = repeat_normalize(sentence, num_repeats=2)
       
    return sentence

def replace_htag(annotation):
    # í•´ì‹œíƒœê·¸ ë°”ê¾¸ê¸°
    annotation = re.sub('#', ', ', annotation)
    return annotation
