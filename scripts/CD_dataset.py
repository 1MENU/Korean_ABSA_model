from util.utils import *
from base_data import *
import re
from soynlp.normalizer import *
from hanspell import spell_checker


def CD_dataset(raw_data, tokenizer, max_len):
    input_ids_list = []
    attention_mask_list = []
    token_type_ids_list = []
    token_labels_list = []

    polarity_input_ids_list = []
    polarity_attention_mask_list = []
    polarity_token_type_ids_list = []
    polarity_token_labels_list = []

    for utterance in raw_data:

        # ì´ ìë¦¬ì— ì „ì²˜ë¦¬ í•  ìˆ˜ ìˆìŒ. utterance['sentence_form'] ë³€í˜•
        # def preprocessing(utterance['sentence_form']) return str
        
        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)
        
        input_ids_list.extend(entity_property_data_dict['input_ids'])
        token_type_ids_list.extend(entity_property_data_dict['token_type_ids'])
        attention_mask_list.extend(entity_property_data_dict['attention_mask'])
        token_labels_list.extend(entity_property_data_dict['label'])

        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])
        polarity_token_type_ids_list.extend(polarity_data_dict['token_type_ids'])
        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])
        polarity_token_labels_list.extend(polarity_data_dict['label'])

    return TensorDataset(
        torch.tensor(input_ids_list),
        torch.tensor(token_type_ids_list),
        torch.tensor(attention_mask_list),
        torch.tensor(token_labels_list)
        ), TensorDataset(
            torch.tensor(polarity_input_ids_list), 
            torch.tensor(polarity_token_type_ids_list),
            torch.tensor(polarity_attention_mask_list),
            torch.tensor(polarity_token_labels_list)
        )


def tokenize_and_align_labels(tokenizer, form, annotations, max_len):

    entity_property_data_dict = {
        'input_ids': [],
        'token_type_ids' : [],
        'attention_mask': [],
        'label': []
    }
    polarity_data_dict = {
        'input_ids': [],
        'token_type_ids' : [],
        'attention_mask': [],
        'label': []
    }

    for pair in entity_property_pair:
        isPairInOpinion = False
        if pd.isna(form):
            break

        # ì´ ìë¦¬ì— ì „ì²˜ë¦¬ ê°€ëŠ¥
        
        form=replace_marks(form)
        pair=replace_htag(pair)
        
        
        sent = pair + tokenizer.cls_token + form
        
        tokenized_data = tokenizer(sent, padding='max_length', max_length=max_len, truncation=True)
        
        for annotation in annotations:
            entity_property = annotation[0]
            polarity = annotation[2]

            # # ë°ì´í„°ê°€ =ë¡œ ì‹œì‘í•˜ì—¬ ìˆ˜ì‹ìœ¼ë¡œ ì¸ì •ëœê²½ìš°
            # if pd.isna(entity) or pd.isna(property):
            #     continue

            if polarity == '------------':
                continue


            if entity_property == pair:
                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])
                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
                entity_property_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
                entity_property_data_dict['label'].append(label_name_to_id['True'])

                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])
                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
                polarity_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
                polarity_data_dict['label'].append(polarity_name_to_id[polarity])

                isPairInOpinion = True
                break

        if isPairInOpinion is False:
            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])
            entity_property_data_dict['token_type_ids'].append(tokenized_data['token_type_ids'])
            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])
            entity_property_data_dict['label'].append(label_name_to_id['False'])

    return entity_property_data_dict, polarity_data_dict


def get_CD_dataset(train_data, dev_data, test_data, pretrained_tokenizer):
    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer)
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

    train_CD_data, train_SC_data = CD_dataset(train_data, tokenizer, 256)
    dev_CD_data, dev_SC_data = CD_dataset(dev_data, tokenizer, 256)
    test_CD_data, test_SC_data = CD_dataset(test_data, tokenizer, 256)

    return train_CD_data, dev_CD_data, test_CD_data

def special_tok_change(sentence):
    #'&name&', '&affiliation&', '&social-security-num&', 
    # '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&'
    sentence=re.sub('&name&','$name$',sentence)
    sentence=re.sub('&affiliation&','$affiliation$',sentence)
    sentence=re.sub('&social-security-num&','$social-security-num$',sentence)
    sentence=re.sub('&tel-num&','$tel-num$',sentence)
    sentence=re.sub('&card-num&','$card-num$',sentence)
    sentence=re.sub('&bank-account&','$bank-account$',sentence)
    sentence=re.sub('&num&','$num$',sentence)
    sentence=re.sub('&online-account&','$online-account$',sentence)
    
    return sentence



def spacing_sent(sentence):
    
    sentence=special_tok_change(sentence) # xml íŒŒì‹± ì‹œì— &ì—ì„œ ì˜¤ë¥˜ë°œìƒí•´ì„œ ë‹¤ ë°”ê¿”ì£¼ê¸°
    sentence=re.sub('&','',sentence)
    result_train = spell_checker.check(sentence)
    sentence = result_train.as_dict()['checked']
    
    return sentence 

def del_emoticon1(sentence):
      # í…ìŠ¤íŠ¸ ì´ëª¨ì§€
    sentence = re.sub('\^\^', '', sentence)
    sentence = re.sub(':\)', '', sentence)
    sentence = re.sub('>.<', '', sentence)
    sentence = re.sub('> 3 <', '', sentence)
    sentence = re.sub('// _ //', '', sentence)
    sentence = re.sub('ã…‹.ã…‹', '', sentence)
    sentence = re.sub('\(--\)\(__\)', '', sentence)
    sentence = re.sub('ğŸ’', 'â¤', sentence)
    sentence = re.sub('ã… ã……ã…œ', '', sentence)
    sentence = re.sub('\:D', '', sentence)
    sentence = re.sub('\+_\+/', '', sentence)
    sentence = re.sub('\^-\^*', '', sentence)
    sentence = re.sub('ã…_ã…', '', sentence)
    sentence= re.sub('-_-', '', sentence)
    sentence=re.sub('ã…‹ã…‹', '', sentence)
    sentence=re.sub('ã…ã…','',sentence)
    sentence=re.sub('ã… ã… ','',sentence)
    sentence=re.sub('ã…œã…œ','',sentence)
    sentence=re.sub('ã…œ','',sentence)
    
    return sentence

def del_emoticon2(sentence):   
    # ğŸ‘ğŸ» ğŸ‘Œ ğŸ¤¡ğŸ‘  ğŸµ ğŸ°ğŸ‚ ğŸ™‹ğŸ» ğŸ™ğŸ» ğ–¤â° ğŸŒ¹ğŸ’‹ğŸ˜²ğŸ–’ğŸ’†â€â™€ğŸ˜¡ğŸ‘Œ ğŸ˜´ğŸ’§ğŸ™†â€â™‚ ğŸ˜ºğŸ™†â€â™‚ğŸ’†ğŸ»â€â™€ğŸ™†ğŸ»ğŸŒ»ğŸ˜®ğŸ¥ğŸŒ \\ devë°ì´í„°ì…‹
    # ğŸŒ¹ ğŸ‘¦ğŸ¼ ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ¤˜ğŸ’¡ğŸ¼ ğŸ˜²ğŸ™ƒğŸ± ğŸ•ºğŸ’ğŸ•·ğŸ•¸ğŸƒâ€â™€âœŒğŸ» ğŸ’‹ğŸ’„ğŸ“¸ğŸ’¯ğŸ’‹ğŸ‘ŒğŸš—ğŸ’¬ ğŸ¤®ğŸµğŸâ° ğŸ‘†ğŸ’ ğŸ·ğŸ˜œ ğŸ™†â€â™‚ğŸ–ğŸ’§ğŸ™‹ğŸ»â€â™€ // train
    # ì´ëª¨ì§€ í†µì¼/ê°ì†Œ 
    sentence=re.sub('ğŸ‘','',sentence)
    sentence=re.sub('ğŸ’•','',sentence)
    sentence=re.sub('ğŸŒ¸','', sentence)
    sentence=re.sub('ğŸ“¸','',sentence)
    sentence = re.sub('ğŸ‘ğŸ»', ''  , sentence)
    sentence = re.sub('ğŸ˜„', ''  , sentence)
    sentence = re.sub('ğŸ–’', ''  , sentence)
    sentence = re.sub('ğŸ‘Œ', ''  , sentence)
    sentence = re.sub('ğŸ¤¡', ''  , sentence)
    sentence = re.sub('ğŸ‘ ', ''  , sentence)
    sentence = re.sub('ğŸµ', ''  , sentence)
    sentence = re.sub('ğŸ°', ''  , sentence)
    sentence = re.sub('ğŸ‚', ''  , sentence)
    sentence = re.sub('ğŸ™‹ğŸ»', ''  , sentence)
    sentence = re.sub('ğŸ™ğŸ»', ''  , sentence)
    sentence = re.sub('ğ–¤', ''  , sentence)
    sentence = re.sub('â°', ''  , sentence)
    sentence = re.sub('ğŸŒ¹', ''  , sentence)
    sentence = re.sub('ğŸ’‹', ''  , sentence)
    sentence = re.sub('ğŸ˜²', ''  , sentence)
    sentence = re.sub('ğŸ’†â€â™€', ''  , sentence)
    sentence = re.sub('ğŸ˜¡', ''  , sentence)
    sentence = re.sub('ğŸ˜´', ''  , sentence)
    sentence = re.sub('ğŸ’§', ''  , sentence)
    sentence = re.sub('ğŸ™†â€â™‚', ''  , sentence)
    sentence = re.sub('ğŸ˜º', ''  , sentence)
    sentence = re.sub('ğŸ’†ğŸ»â€â™€', ''  , sentence)
    sentence = re.sub('ğŸ™†ğŸ»', ''  , sentence)
    sentence = re.sub('ğŸŒ»', ''  , sentence)
    sentence = re.sub('ğŸ˜®', ''  , sentence)
    sentence = re.sub('ğŸ¥', ''  , sentence)
    sentence = re.sub('ğŸŒ', ''  , sentence)
    sentence = re.sub('ğŸ‘¦ğŸ¼', ''  , sentence)
    sentence = re.sub('ğŸ‘ğŸ»', ''  , sentence)
    sentence = re.sub('ğŸ¤˜', ''  , sentence)
    sentence = re.sub('ğŸ’¡', ''  , sentence)
    sentence = re.sub('ğŸ¼', ''  , sentence)
    sentence = re.sub('ğŸ˜²', ''  , sentence)
    sentence = re.sub('ğŸ™ƒ', ''  , sentence)
    sentence = re.sub('ğŸ±', ''  , sentence)
    sentence = re.sub('ğŸ•º', ''  , sentence)
    sentence = re.sub('ğŸ•·', ''  , sentence)
    sentence = re.sub('ğŸ•¸', ''  , sentence)
    sentence = re.sub('ğŸƒâ€â™€', ''  , sentence)
    sentence = re.sub('âœŒğŸ»', ''  , sentence)
    sentence = re.sub('ğŸ’¯', ''  , sentence)
    sentence = re.sub('ğŸ¤®', ''  , sentence)
    sentence = re.sub('ğŸ˜œ', ''  , sentence)
    sentence = re.sub('ğŸ–', ''  , sentence)
    
    return sentence

def replace_htag(sentence): # annotation í•´ì‹œ ì œê±° ìš© 
    # í•´ì‹œíƒœê·¸ ë°”ê¾¸ê¸°    #ë¬¸ì¥ë‚´ì—ì„œëŠ” í•´ì‹œíƒœê·¸ ê³µë°±ìœ¼ë¡œ ë°”ê¿”ì£¼ê³  ì†ì„± ë²”ì£¼ì—ì„œëŠ” #->, ë¡œ ë°”ê¿”ì£¼ê¸° 
    sentence = re.sub('#', ', ', sentence)
    return sentence

def repeat_del(sentence): #ì˜ë¯¸ì—†ëŠ” ë°˜ë³µ ì œê±° í•¨ìˆ˜ 
    sentence=repeat_normalize(sentence, num_repeats=2)   
    return sentence

def replace_marks(sentence):

    sentence=spacing_sent(sentence)
    # í…ìŠ¤íŠ¸ì´ëª¨í‹°ì½˜ ì œê±° 
    sentence=del_emoticon1(sentence)
    # ì´ëª¨í‹°ì½˜ ì œê±° 
    sentence=del_emoticon2(sentence)
    # í•´ì‹œíƒœê·¸ ë°”ê¾¸ê¸°
    sentence=sentence = re.sub('#', '', sentence)
    #ë°˜ë³µì œê±° 
    sentence=repeat_del(sentence)

                    
    return sentence
